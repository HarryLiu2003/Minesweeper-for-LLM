# Minesweeper LLM Experiment Metrics Documentation

This document details the metrics calculated by the `analyze_results.py` script for the Minesweeper LLM experiments and assesses their alignment with the definitions provided in the paper "Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study".

## Overview

The `analyze_results.py` script processes the structured JSON output files generated by the modified `tasks/ms.py` script (which now includes the final board state). It aggregates data across multiple game runs to calculate overall performance benchmarks. The script requires access to both the output directory (containing results) and the corresponding input directory (containing the ground truth board data).

## Metric Calculation and Alignment

Below is a breakdown of each objective metric calculated by the script:

### 1. % Solved

*   **Script Calculation:** Counts a game as solved if, at the end of the game, the set of flagged cells exactly matches the set of actual mine locations. This is determined by: (1) Reading the saved `final_board_disp` from the output JSON to find all cells marked with the flag symbol ('F'). (2) Comparing this set of actual `final_flags` to the `ground_truth_mines` (derived from the `board_mine` matrix in the input JSON). (3) The game is solved if `correctly_flagged_mines == total_mines` and `incorrectly_flagged_non_mines == 0`. The final percentage is `(total solved games / total processed games) * 100`.
*   **Paper Definition (Sec 5.2):** "A game is deemed 'solved' when all mines are accurately marked and no extraneous cells are flagged."
*   **Alignment:** **High.** The script now uses the actual final board state and directly implements the strict definition provided in the paper.

### 2. % Failed

*   **Script Calculation:** Counts a game as failed if any action within that game received the `ActionFeedbackCodes.GAME_OVER` (value 10) feedback code. Games marked as 'solved' using the strict definition above are explicitly *not* counted as failed. The final percentage is `(total failed games / total processed games) * 100`.
*   **Paper Definition (Sec 5.2 & 3):** Failure occurs upon "erroneous left-click on mines or incorrect flag placements during middle-click verifications."
*   **Alignment:** **High.** The `GAME_OVER` feedback code corresponds to these failure conditions.

### 3. % Flagged

*   **Script Calculation:** Reflects the proportion of actual mines that were correctly flagged at the end of all analyzed games. Calculated as `(total correctly flagged mines across all games / total actual mines across all games) * 100`. The script determines the actual `final_flags` by reading the saved `final_board_disp`. It then compares this set against the `ground_truth_mines` for each game to count `correctly_flagged_mines`. Requires corresponding input JSON files and the `final_board_disp` key in output files.
*   **Paper Definition (Sec 5.2):** "The metric for correctly flagged mines is based on the quantity of mines identified by the model that align with the actual mine locations at the game's conclusion."
*   **Alignment:** **High.** The script now uses the actual final board state, directly matching the definition.

### 4. % Valid

*   **Script Calculation:** Counts the total number of actions across all games (excluding the first action of each game) that received the `ActionFeedbackCodes.SUCCESS` (value 0) feedback code. The percentage is `(total valid actions / total actions excluding initial ones) * 100`.
*   **Paper Definition (Sec 5.2 & Table 2 Caption):** "A 'valid' action is one that advances gameplay without activating a mine." "The initial action, L(3,3), is consistently excluded from the count."
*   **Alignment:** **High.** The interpretation of `SUCCESS` feedback aligns with the definition, and the script correctly excludes the initial action.

### 5. % Repeated

*   **Script Calculation:** Counts the total number of actions across all games (excluding the first action of each game) whose feedback code is present in the refined `REPEATED_FEEDBACK_CODES` set (codes 2, 3, 4, 5, 6, 7, 8, 9, 12). This represents attempts to act on cells in invalid states (e.g., clicking revealed cells, flagging revealed cells). The percentage is `(total repeated actions / total actions excluding initial ones) * 100`.
*   **Paper Definition (Sec 5.3 & Table 2 Caption):** Discusses "repetitive actions" like acting on cells in invalid states. Mentions exclusion of initial action.
*   **Alignment:** **High.** The script identifies actions ineffective due to cell state, matching the concept. Uses the refined set of codes and excludes the initial action. The exact set of codes considered "repeated" by the paper might still differ slightly, but this interpretation is well-aligned.

## Metrics Not Calculated

*   **% Accurate and Coherent Logic:** Requires manual inspection (Paper Sec 5.2) and is **not** calculated by the script.

## Dependencies and Assumptions

*   **Input Files:** Requires output JSONs (from `tasks/ms.py` modified to include `final_board_disp`) and corresponding input JSONs (from `data/...` containing `board_mine` matrix).
*   **Output File Structure:** Assumes output JSONs contain structured `action_history` (list of dicts with `action_str`, `feedback`) and `final_board_disp` (list of lists).
*   **Feedback Codes:** Relies on accuracy of `ActionFeedbackCodes` constants based on `src/game/core.py`.
*   **Flag Symbol:** Assumes the flag symbol in `final_board_disp` is 'F'.

## Usage

Run the script from the project root directory:

```bash
# Ensure conda environment is active: conda activate ms
python analyze_results.py <path_to_output_dir> <path_to_input_dir>
```

Example:
```bash
python analyze_results.py ./output/board-solve/5x5-4-coord-ch/ ./data/5x5-4-labeled/
```

## Replication Journey and Challenges

This section summarizes the iterative process undertaken to align the `analyze_results.py` script with the metrics reported in the original paper, highlighting successes and remaining discrepancies.

**Process Summary:**

1.  **Initial Analysis:** An initial script was developed based on the paper's metric descriptions and assumptions about the output data format.
2.  **Output Format Correction (Feedback):** It was discovered that the original `tasks/ms.py` did not save the crucial `ActionFeedback` code alongside each action in the output JSON. `tasks/ms.py` was modified to store a structured `action_history` including both `action_str` and `feedback` value.
3.  **Input Format Correction (Ground Truth):** The analysis script initially assumed ground truth mines were stored as a list of coordinates (`mine_locations`) in the input board files. Examination revealed they were stored as a `board_mine` matrix. `analyze_results.py` was updated to parse this matrix correctly.
4.  **Debugging:** Several iterations of debugging were required for the analysis script, particularly resolving variable scope issues (`UnboundLocalError`) related to aggregating total mine counts.
5.  **Output Format Correction (Final State):** Analysis revealed that accurately calculating `% Flagged` and the strict `% Solved` condition required knowing the final state of the game board flags. The initial simulation based only on action history was insufficient. `tasks/ms.py` was modified again to save the `final_board_disp` list in the output JSON.
6.  **Analysis Refinement (Flags & Solved):** `analyze_results.py` was updated to read `final_board_disp` and use the actual final flag positions for calculating `% Flagged` and the strict `% Solved` metric.
7.  **Analysis Refinement (Action Metrics):** The script was adjusted to exclude the initial `L(3,3)` action from the counts for `% Valid` and `% Repeated` actions, aligning with the paper's Table 2 caption. The definition of `REPEATED_FEEDBACK_CODES` was also slightly refined.

**Successes:**

*   The iterative process successfully identified and corrected issues related to missing data (feedback codes, final board state) in the output.
*   Ground truth mine locations are now correctly parsed.
*   Metrics like `% Solved` (Strict) and `% Flagged` achieved high alignment with the paper's results after using the actual final board state.
*   `% Failed` and `Total Actions` are reasonably close, with minor differences likely attributable to stochastic factors or edge-case handling.

**Remaining Challenges & Discrepancies:**

*   **`% Valid` and `% Repeated`:** Despite refinements, these metrics still show significant deviation from the paper's Table 2 values (e.g., for 3.5-16k CH Coord, Script: Valid ~15%, Repeated ~81%; Paper: Valid ~23%, Repeated ~45%).
*   **Hypothesized Reason:** The most likely cause is a difference in the precise categorization of non-SUCCESS, non-GAME_OVER feedback codes. The paper might have used a narrower definition of "Repeated" (perhaps only specific action types or consecutive identical actions) or a broader definition of "Valid" (potentially including certain non-failing error codes). Access to the paper's specific categorization rules (potentially detailed in their referenced, but not included, "Table 3") would be needed to fully resolve this.

**Conclusion:**

The current `analyze_results.py` script represents a robust implementation based on the available source code and the paper's explicit definitions. It accurately calculates most key metrics. The remaining discrepancies in action validity metrics are acknowledged and likely stem from subtle definitional differences in the original paper's analysis process.
